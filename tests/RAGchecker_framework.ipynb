{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4b67a0",
   "metadata": {},
   "source": [
    "# RAG Pipeline rag-checker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca28065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 21:36:38,163 | INFO | Loading faiss with AVX2 support.\n",
      "2025-08-04 21:36:38,323 | INFO | Successfully loaded faiss with AVX2 support.\n",
      "2025-08-04 21:36:38,335 | INFO | Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT - c:\\Users\\insung\\Finance_Agent\n",
      "환경 변수 로드 완료\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import sys, pandas as pd\n",
    "\n",
    "ROOT = Path.cwd().parent  # 노트북은 tests/ 하위에 있다고 가정\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "from services.orchestrator import router_node\n",
    "from ragas import SingleTurnSample\n",
    "\n",
    "from config import OPENAI_API_KEY\n",
    "\n",
    "load_dotenv(ROOT / \".env\")\n",
    "print(f\"ROOT - {ROOT}\")\n",
    "print(\"환경 변수 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f67c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0804 21:23:51.085000 25528 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag-checker 초기화 완료\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "from ragchecker import RAGChecker\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi() \n",
    "\n",
    "def openai_api_function(prompts: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    OpenAI의 gpt-4o 모델을 호출하고 응답 텍스트 리스트를 반환하는 커스텀 함수.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # litellm을 사용하여 OpenAI 모델을 배치로 호출합니다.\n",
    "        response = litellm.batch_completion(\n",
    "            model=\"gpt-4o\",  # 사용할 OpenAI 모델 지정\n",
    "            messages=[[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
    "        )\n",
    "        \n",
    "        return [res.choices[0].message.content for res in response]\n",
    "    except Exception as e:\n",
    "        print(f\"API 호출 중 에러 발생: {e}\")\n",
    "        # 에러 발생 시, 각 프롬프트에 대해 빈 문자열을 반환하여 평가가 중단되지 않게 함\n",
    "        return [\"\" for _ in prompts]\n",
    "\n",
    "class KiwiTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def tokenize(self, text):\n",
    "        return [token.form for token in kiwi.tokenize(text)]\n",
    "    def lemmatize(self, text):\n",
    "        return [token.form for token in kiwi.tokenize(text)]\n",
    "\n",
    "checker = RAGChecker(\n",
    "    tokenizer=KiwiTokenizer(),\n",
    "    language=\"ko\",\n",
    "    custom_llm_api_func=openai_api_function\n",
    ")\n",
    "\n",
    "print(\"rag-checker 초기화 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20713e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 질문 정의\n",
    "SAMPLE_QUESTIONS = [\n",
    "    \"OTP 비밀번호 오류 해제 방법 알려줘\",\n",
    "    \"첫급여 우리적금에서 우대이율을 받기 위한 조건은 무엇인가요?\",\n",
    "    \"정기적금을 만기 지난 뒤 해지하면 어떤 만기후이율이 적용되나요?\",\n",
    "    \"오늘 날씨 어때?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc5e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4개 레코드 수집 완료\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 호출 → 평가 입력 변환\n",
    "from services.orchestrator import router_node\n",
    "\n",
    "records = []\n",
    "for q in SAMPLE_QUESTIONS:\n",
    "    res  = router_node.invoke(q)\n",
    "    ctxs = res.get(\"context\", \"\").split(\"\\n\\n\") if res.get(\"context\") else []\n",
    "    records.append({\"question\": q, \"answer\": res[\"answer\"], \"contexts\": ctxs})\n",
    "print(f\"{len(records)}개 레코드 수집 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0af33",
   "metadata": {},
   "source": [
    "## RAGchecker Evaluavtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d724ba36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGResults(\n",
      "  4 RAG results,\n",
      "  Metrics:\n",
      "  {\n",
      "    \"overall_metrics\": {},\n",
      "    \"retriever_metrics\": {},\n",
      "    \"generator_metrics\": {}\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from ragchecker import RAGResult, RAGResults\n",
    "\n",
    "rag_result_objects = []\n",
    "for i, record in enumerate(records):\n",
    "    rag_result_objects.append(\n",
    "        RAGResult(\n",
    "            query_id=f\"q_{i}\",\n",
    "            query=record[\"question\"],\n",
    "            response=record[\"answer\"],\n",
    "            retrieved_context=record[\"contexts\"],\n",
    "            # gt_answer는 실제 정답이 있을 경우 제공, 없으면 None\n",
    "            gt_answer=None \n",
    "        )\n",
    "    )\n",
    "\n",
    "rag_results = RAGResults(results=rag_result_objects)\n",
    "\n",
    "print(rag_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf02c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-04 21:25:31.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragchecker.evaluator\u001b[0m:\u001b[36mextract_claims\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1mExtracting claims for response of 4 RAG results.\u001b[0m\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[92m21:25:31 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= meta.llama3-70b-instruct-v1:0; provider = bedrock\n",
      "2025-08-04 21:25:31,279 | INFO | \n",
      "LiteLLM completion() model= meta.llama3-70b-instruct-v1:0; provider = bedrock\n",
      "\u001b[92m21:25:31 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= meta.llama3-70b-instruct-v1:0; provider = bedrock\n",
      "2025-08-04 21:25:31,303 | INFO | \n",
      "LiteLLM completion() model= meta.llama3-70b-instruct-v1:0; provider = bedrock\n",
      "\u001b[92m21:25:31 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= meta.llama3-70b-instruct-v1:0; provider = bedrock\n",
      "2025-08-04 21:25:31,317 | INFO | \n",
      "LiteLLM completion() model= meta.llama3-70b-instruct-v1:0; provider = bedrock\n",
      "\u001b[92m21:25:31 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= meta.llama3-70b-instruct-v1:0; provider = bedrock\n",
      "2025-08-04 21:25:31,326 | INFO | \n",
      "LiteLLM completion() model= meta.llama3-70b-instruct-v1:0; provider = bedrock\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AuthenticationError' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Overall Metrics (Precision / Recall / F1)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m overall = \u001b[43mchecker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrag_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m### Overall ###\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m overall.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\insung\\Finance_Agent\\.venv\\Lib\\site-packages\\ragchecker\\evaluator.py:228\u001b[39m, in \u001b[36mRAGChecker.evaluate\u001b[39m\u001b[34m(self, results, metrics, save_path)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# compute the required intermediate results\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m requirement \u001b[38;5;129;01min\u001b[39;00m requirements:\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_claims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m save_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    230\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\insung\\Finance_Agent\\.venv\\Lib\\site-packages\\ragchecker\\evaluator.py:146\u001b[39m, in \u001b[36mRAGChecker.check_claims\u001b[39m\u001b[34m(self, results, check_type)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33manswer2response\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    145\u001b[39m     results = [ret \u001b[38;5;28;01mfor\u001b[39;00m ret \u001b[38;5;129;01min\u001b[39;00m results.results \u001b[38;5;28;01mif\u001b[39;00m ret.answer2response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_claims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     claims = [ret.response_claims \u001b[38;5;28;01mfor\u001b[39;00m ret \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[32m    148\u001b[39m     references = [ret.gt_answer \u001b[38;5;28;01mfor\u001b[39;00m ret \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\insung\\Finance_Agent\\.venv\\Lib\\site-packages\\ragchecker\\evaluator.py:114\u001b[39m, in \u001b[36mRAGChecker.extract_claims\u001b[39m\u001b[34m(self, results, extract_type)\u001b[39m\n\u001b[32m    111\u001b[39m questions = [result.query \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[32m    113\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracting claims for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m RAG results.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m extraction_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_responses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_questions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextractor_max_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43msagemaker_client\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msagemaker_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43msagemaker_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msagemaker_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43msagemaker_get_response_func\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msagemaker_get_response_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_api_func\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcustom_llm_api_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m claims = [[c.content \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m res.claims] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m extraction_results]\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\insung\\Finance_Agent\\.venv\\Lib\\site-packages\\refchecker\\extractor\\extractor_base.py:26\u001b[39m, in \u001b[36mExtractorBase.extract\u001b[39m\u001b[34m(self, batch_responses, batch_questions, max_new_tokens, sagemaker_client, sagemaker_params, sagemaker_get_response_func, custom_llm_api_func, **kwargs)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract\u001b[39m(\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mself\u001b[39m, \n\u001b[32m     16\u001b[39m     batch_responses, \n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     **kwargs\n\u001b[32m     24\u001b[39m ):\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.claim_format == \u001b[33m'\u001b[39m\u001b[33mtriplet\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_claim_triplets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_responses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_responses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_questions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_questions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43msagemaker_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43msagemaker_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43msagemaker_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msagemaker_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m            \u001b[49m\u001b[43msagemaker_get_response_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43msagemaker_get_response_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcustom_llm_api_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_api_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.claim_format == \u001b[33m'\u001b[39m\u001b[33msubsentence\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     37\u001b[39m         result = \u001b[38;5;28mself\u001b[39m.extract_subsentence_claims(\n\u001b[32m     38\u001b[39m             batch_responses=batch_responses,\n\u001b[32m     39\u001b[39m             batch_questions=batch_questions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m             **kwargs\n\u001b[32m     46\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\insung\\Finance_Agent\\.venv\\Lib\\site-packages\\refchecker\\extractor\\llm_extractor.py:146\u001b[39m, in \u001b[36mLLMExtractor.extract_claim_triplets\u001b[39m\u001b[34m(self, batch_responses, batch_questions, max_new_tokens, sagemaker_client, sagemaker_params, sagemaker_get_response_func, custom_llm_api_func, **kwargs)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(prompt_list), \u001b[38;5;28mself\u001b[39m.batch_size)):\n\u001b[32m    144\u001b[39m     batch_prompts = prompt_list[_i:_i+\u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     llm_responses = \u001b[43mget_model_batch_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_choices\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43msagemaker_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43msagemaker_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43msagemaker_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msagemaker_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43msagemaker_get_response_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43msagemaker_get_response_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_api_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_api_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m llm_responses \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(llm_responses):\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m llm_responses:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\insung\\Finance_Agent\\.venv\\Lib\\site-packages\\refchecker\\utils.py:126\u001b[39m, in \u001b[36mget_model_batch_response\u001b[39m\u001b[34m(prompts, model, temperature, n_choices, max_new_tokens, api_base, sagemaker_client, sagemaker_params, sagemaker_get_response_func, custom_llm_api_func, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response_list\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_api_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcustom_llm_api_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m     message_list = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mmy_llm_api_function\u001b[39m\u001b[34m(prompts)\u001b[39m\n\u001b[32m     14\u001b[39m response = litellm.batch_completion(\n\u001b[32m     15\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mbedrock/meta.llama3-70b-instruct-v1:0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     messages=[[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: p}] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# litellm 응답에서 텍스트만 추출하여 리스트로 반환합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m[\u001b[32m0\u001b[39m].message.content \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m response]\n",
      "\u001b[31mAttributeError\u001b[39m: 'AuthenticationError' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "# Overall Metrics (Precision / Recall / F1)\n",
    "\n",
    "overall = checker.evaluate(rag_results)\n",
    "print(\"### Overall ###\")\n",
    "for k, v in overall.items():\n",
    "    print(f\"{k:<10}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9507e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic Retriever Metrics (top-k 정밀·재현율 등)\n",
    "\n",
    "diag_retr = checker.diagnose_retriever(rag_results, k_values=[1,3,5])\n",
    "print(\"\\n### Retriever Diagnostics ###\")\n",
    "display(diag_retr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da53b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic Generator Metrics\n",
    "\n",
    "diag_gen = checker.diagnose_generator(rag_results)\n",
    "print(\"### Generator Diagnostics ###\")\n",
    "display(diag_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claim-level Entailment\n",
    "\n",
    "entail = checker.check_entailment(rag_results)\n",
    "df_ent = pd.DataFrame(entail)\n",
    "print(\"### Claim-level Entailment ###\")\n",
    "display(df_ent.head())\n",
    "print(\"\\nEntailment accuracy :\", df_ent[\"entailment\"].mean().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b106b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
