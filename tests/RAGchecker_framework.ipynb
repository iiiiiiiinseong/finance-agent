{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4b67a0",
   "metadata": {},
   "source": [
    "# RAG Pipeline rag-checker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca28065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 22:17:28,613 | INFO | Loading faiss with AVX2 support.\n",
      "2025-08-04 22:17:28,760 | INFO | Successfully loaded faiss with AVX2 support.\n",
      "2025-08-04 22:17:28,771 | INFO | Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT - c:\\Users\\insung\\Finance_Agent\n",
      "환경 변수 로드 완료\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import sys, pandas as pd\n",
    "\n",
    "ROOT = Path.cwd().parent  # 노트북은 tests/ 하위에 있다고 가정\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "from services.orchestrator import router_node\n",
    "from ragas import SingleTurnSample\n",
    "\n",
    "load_dotenv(ROOT / \".env\")\n",
    "print(f\"ROOT - {ROOT}\")\n",
    "print(\"환경 변수 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f67c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0804 22:17:53.102000 15316 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag-checker 초기화 완료\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "from ragchecker import RAGChecker\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi() \n",
    "\n",
    "def openai_api_function(prompts: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    OpenAI의 gpt-4o 모델을 호출하고 응답 텍스트 리스트를 반환하는 커스텀 함수.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # litellm을 사용하여 OpenAI 모델을 배치로 호출합니다.\n",
    "        response = litellm.batch_completion(\n",
    "            model=\"gpt-4o\",  # 사용할 OpenAI 모델 지정\n",
    "            messages=[[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
    "        )\n",
    "        \n",
    "        return [res.choices[0].message.content for res in response]\n",
    "    except Exception as e:\n",
    "        print(f\"API 호출 중 에러 발생: {e}\")\n",
    "        # 에러 발생 시, 각 프롬프트에 대해 빈 문자열을 반환하여 평가가 중단되지 않게 함\n",
    "        return [\"\" for _ in prompts]\n",
    "\n",
    "class KiwiTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def tokenize(self, text):\n",
    "        return [token.form for token in kiwi.tokenize(text)]\n",
    "    def lemmatize(self, text):\n",
    "        return [token.form for token in kiwi.tokenize(text)]\n",
    "\n",
    "checker = RAGChecker(\n",
    "    tokenizer=KiwiTokenizer(),\n",
    "    language=\"ko\",\n",
    "    custom_llm_api_func=openai_api_function\n",
    ")\n",
    "\n",
    "print(\"rag-checker 초기화 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20713e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 질문 정의\n",
    "SAMPLE_QUESTIONS = [\n",
    "    \"OTP 비밀번호 오류 해제 방법 알려줘\",\n",
    "    \"첫급여 우리적금에서 우대이율을 받기 위한 조건은 무엇인가요?\",\n",
    "    \"정기적금을 만기 지난 뒤 해지하면 어떤 만기후이율이 적용되나요?\",\n",
    "    \"오늘 날씨 어때?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc5e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4개 레코드 수집 완료\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 호출 → 평가 입력 변환\n",
    "from services.orchestrator import router_node\n",
    "\n",
    "records = []\n",
    "for q in SAMPLE_QUESTIONS:\n",
    "    res  = router_node.invoke(q)\n",
    "    ctxs = res.get(\"context\", \"\").split(\"\\n\\n\") if res.get(\"context\") else []\n",
    "    records.append({\"question\": q, \"answer\": res[\"answer\"], \"contexts\": ctxs})\n",
    "print(f\"{len(records)}개 레코드 수집 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0af33",
   "metadata": {},
   "source": [
    "## RAGchecker Evaluavtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d724ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from ragchecker import RAGChecker, RAGResult, RAGResults\n",
    "from ragchecker.metrics import all_metrics\n",
    "\n",
    "rag_result_objects = []\n",
    "for i, record in enumerate(records):\n",
    "    structured_contexts = [\n",
    "        SimpleNamespace(doc_id=f\"doc_{i}_{j}\", text=ctx) \n",
    "        for j, ctx in enumerate(record[\"contexts\"])\n",
    "    ]\n",
    "\n",
    "    rag_result_objects.append(\n",
    "        RAGResult(\n",
    "            query_id=f\"q_{i}\",\n",
    "            query=record[\"question\"],\n",
    "            response=record[\"answer\"],\n",
    "            retrieved_context=structured_contexts,\n",
    "            gt_answer=None              # gt_answer는 실제 정답이 있을 경우 제공, 없으면 None\n",
    "        )\n",
    "    )\n",
    "\n",
    "rag_results = RAGResults(results=rag_result_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cf02c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-04 22:36:40.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragchecker.evaluator\u001b[0m:\u001b[36mextract_claims\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1mExtracting claims for response of 4 RAG results.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 1. Faithfulness (Generator) Evaluation ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:40,472 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:40,479 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:40,484 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:40,492 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:41 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:41,695 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:42,172 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:42,280 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:42,504 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "\u001b[32m2025-08-04 22:36:42.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mragchecker.evaluator\u001b[0m:\u001b[36mcheck_claims\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mChecking retrieved2response for 4 RAG results.\u001b[0m\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,525 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,549 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,554 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,559 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,569 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,577 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,582 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,591 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,597 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,602 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,610 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,618 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,640 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,646 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,651 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,671 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,707 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,711 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,719 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,728 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,748 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3260 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,753 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,759 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,767 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-08-04 22:36:42,775 | INFO | \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,188 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,305 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,312 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,321 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,322 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,340 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,353 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,398 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,445 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,448 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,486 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,493 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,512 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,532 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,546 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,549 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,565 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,589 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,613 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,622 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,629 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,655 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,662 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:43,685 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:36:44 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "2025-08-04 22:36:44,487 | INFO | Wrapper: Completed Call, calling success_handler\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_metrics: {}\n",
      "retriever_metrics: {}\n",
      "generator_metrics: {'faithfulness': np.float64(25.0)}\n",
      "\n",
      "--- 평가 완료 ---\n",
      "Faithfulness Score (0-100점, 높을수록 좋음):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_metrics</th>\n",
       "      <th>retriever_metrics</th>\n",
       "      <th>generator_metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'faithfulness': 25.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  overall_metrics retriever_metrics       generator_metrics\n",
       "0              {}                {}  {'faithfulness': 25.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 정답 텍스트가 없는 경우\n",
    "print(\"\\n### 1. Faithfulness (Generator) Evaluation ###\")\n",
    "metrics_to_run = [\"faithfulness\"]\n",
    "overall = checker.evaluate(rag_results, metrics=metrics_to_run)\n",
    "for k, v in overall.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 평가 완료 ---\")\n",
    "print(\"Faithfulness Score (0-100점, 높을수록 좋음):\")\n",
    "display(pd.DataFrame([overall]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b106b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87915b5d",
   "metadata": {},
   "source": [
    "현재는 정답을 따로 구축하지 않아서 위의 코드블럭에서와 같이 'faithfulness'만 평가 함. \\\n",
    "이는 모델이 생성한 답변이 검색된 문맥에 얼마나 충실하게 근거하고 있는지를 평가하는 것으로 환각을 측정하는 것임.\n",
    "\\\n",
    "\\\n",
    "정답 텍스트를 만들었을 경우 아래의 코드 청크에 올바르게 입력한 뒤 위에서 확인하지 못 했던 overall_metrics, retriever_merics를 평가할 수 있음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660ce35d",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 정답 텍스트가 있는 경우\n",
    "\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "from ragchecker import RAGChecker, RAGResult, RAGResults\n",
    "from ragchecker.metrics import all_metrics # 모든 지표를 임포트\n",
    "\n",
    "# --- 데이터 준비 (gt_answer에 실제 정답 추가) ---\n",
    "# 실제 평가를 위해서는 여기에 '모범 정답'과 '정답 문서 ID'를 넣어야함.\n",
    "# 아래는 예시 데이터.\n",
    "ground_truth_data = [\n",
    "    {\"answer\": \"우리WON뱅킹에서 비대면 실명확인을 통해 해제할 수 있습니다. 인터넷뱅킹에서는 불가능합니다.\", \"context_ids\": [\"doc_0_0\"]}, # 가상의 정답\n",
    "    {\"answer\": \"첫급여 수령, 마케팅 동의 등의 조건을 충족하면 최대 연 4%의 우대금리를 받을 수 있습니다.\", \"context_ids\": [\"doc_1_x\"]}, # 가상의 정답\n",
    "    {\"answer\": \"만기 후 기간에 따라 약정된 만기후이율이 적용되며, 보통 만기 시점보다 낮은 금리입니다.\", \"context_ids\": [\"doc_2_y\"]}, # 가상의 정답\n",
    "    {\"answer\": \"날씨 정보는 제공하지 않습니다.\", \"context_ids\": []} # 가상의 정답\n",
    "]\n",
    "\n",
    "rag_result_objects = []\n",
    "for i, record in enumerate(records):\n",
    "    structured_contexts = [\n",
    "        SimpleNamespace(doc_id=f\"doc_{i}_{j}\", text=ctx) \n",
    "        for j, ctx in enumerate(record[\"contexts\"])\n",
    "    ]\n",
    "    rag_result_objects.append(\n",
    "        RAGResult(\n",
    "            query_id=f\"q_{i}\",\n",
    "            query=record[\"question\"],\n",
    "            response=record[\"answer\"],\n",
    "            retrieved_context=structured_contexts,\n",
    "            gt_answer=ground_truth_data[i][\"answer\"], # 실제 정답 텍스트 제공\n",
    "            gt_context_ids=ground_truth_data[i][\"context_ids\"] # 실제 정답 문서 ID 제공\n",
    "        )\n",
    "    )\n",
    "rag_results = RAGResults(results=rag_result_objects)\n",
    "\n",
    "# --- 평가 실행 ---\n",
    "print(\"\\n### 정답(GT) 포함 전체 파이프라인 평가 시작 ###\")\n",
    "try:\n",
    "    # 'all_metrics'를 사용하여 모든 지표를 한 번에 평가\n",
    "    checker.evaluate(rag_results, metrics=all_metrics)\n",
    "\n",
    "    print(\"\\n--- 평가 완료 ---\")\n",
    "    # 평가 결과는 rag_results.metrics 속성에 저장됩니다.\n",
    "    \n",
    "    print(\"\\n### 종합 평가 (Overall Metrics) ###\")\n",
    "    display(pd.DataFrame([rag_results.metrics[\"overall_metrics\"]]))\n",
    "\n",
    "    print(\"\\n### 검색기 평가 (Retriever Metrics) ###\")\n",
    "    display(pd.DataFrame([rag_results.metrics[\"retriever_metrics\"]]))\n",
    "    \n",
    "    print(\"\\n### 생성기 평가 (Generator Metrics) ###\")\n",
    "    display(pd.DataFrame([rag_results.metrics[\"generator_metrics\"]]))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"평가 중 에러 발생: {e}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
